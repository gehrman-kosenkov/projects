{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from googleapiclient.discovery import build\n",
    "from google_auth_oauthlib.flow import InstalledAppFlow\n",
    "from google.auth.transport.requests import Request\n",
    "import datetime\n",
    "import pickle\n",
    "import os.path\n",
    "import base64\n",
    "import pandas as pd\n",
    "from io import BytesIO\n",
    "import os\n",
    "import openpyxl\n",
    "import mysql.connector\n",
    "from sshtunnel import SSHTunnelForwarder\n",
    "import pymysql\n",
    "import mysql.connector\n",
    "from requests.structures import CaseInsensitiveDict\n",
    "import requests\n",
    "\n",
    "# If modifying these SCOPES, delete the file token.pickle.\n",
    "SCOPES = ['https://www.googleapis.com/auth/gmail.readonly']\n",
    "\n",
    "def get_gmail_service():\n",
    "    creds = None\n",
    "    # The file token.pickle stores the user's access and refresh tokens.\n",
    "    if os.path.exists('token.pickle'):\n",
    "        with open('token.pickle', 'rb') as token:\n",
    "            creds = pickle.load(token)\n",
    "    # If there are no (valid) credentials available, let the user log in.\n",
    "    if not creds or not creds.valid:\n",
    "        if creds and creds.expired and creds.refresh_token:\n",
    "            creds.refresh(Request())\n",
    "        else:\n",
    "            flow = InstalledAppFlow.from_client_secrets_file(\n",
    "                'directory', SCOPES)\n",
    "            creds = flow.run_local_server(port=0)\n",
    "        # Save the credentials for the next run\n",
    "        with open('token.pickle', 'wb') as token:\n",
    "            pickle.dump(creds, token)\n",
    "\n",
    "    service = build('gmail', 'v1', credentials=creds)\n",
    "    return service\n",
    "\n",
    "#Searching for specific emails containing needed files\n",
    "def search_messages(service, query):\n",
    "    result = service.users().messages().list(userId='me', q=query).execute()\n",
    "    messages = []\n",
    "    if 'messages' in result:\n",
    "        messages.extend(result['messages'])\n",
    "    return messages\n",
    "\n",
    "#retrieving an excel file\n",
    "def get_attachments(service, message_id, store_dir, desired_filename):\n",
    "    try:\n",
    "        message = service.users().messages().get(userId='me', id=message_id).execute()\n",
    "        parts = message.get('payload', {}).get('parts', [])\n",
    "        for part in parts:\n",
    "            if part['filename'] == desired_filename:  # Check if the filename matches the desired one\n",
    "                if 'data' in part['body']:\n",
    "                    data = part['body']['data']\n",
    "                else:\n",
    "                    att_id = part['body'].get('attachmentId')\n",
    "                    att = service.users().messages().attachments().get(userId='me', messageId=message_id, id=att_id).execute()\n",
    "                    data = att['data']\n",
    "\n",
    "                file_data = base64.urlsafe_b64decode(data.encode('UTF-8'))\n",
    "                path = os.path.join(store_dir, part['filename'])\n",
    "\n",
    "                if not os.path.exists(store_dir):\n",
    "                    os.makedirs(store_dir)\n",
    "                with open(path, 'wb') as f:\n",
    "                    f.write(file_data)\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "\n",
    "current_date = datetime.date.today()\n",
    "current_week = current_date.isocalendar()[1]\n",
    "previous_week = current_week - 1\n",
    "\n",
    "current_year = datetime.date.today().year\n",
    "\n",
    "new_file_path = ''\n",
    "\n",
    "\n",
    "def main():\n",
    "    service = get_gmail_service()\n",
    "    search_query = 'from:XXXX.org has:attachment filename:XXXX.xlsx'\n",
    "    messages = search_messages(service, search_query)\n",
    "\n",
    "    if messages:\n",
    "        latest_message = messages[0]  # Get the latest email\n",
    "\n",
    "        # Construct the path for saving the attachment with the week and year in the file name\n",
    "        folder_name = f'{current_year}_Week_{previous_week}'\n",
    "        save_path = 'x'\n",
    "        \n",
    "\n",
    "        # Call get_attachments with the desired filename\n",
    "        get_attachments(service, latest_message['id'], save_path, 'XXXX.xlsx')\n",
    "\n",
    "        # Path to the downloaded Excel file\n",
    "        old_file_path = os.path.join(save_path, 'XXXX.xlsx')\n",
    "        new_file_path = os.path.join(save_path, f'{current_year}_Week_{previous_week}.xlsx')\n",
    "\n",
    "        try:\n",
    "            # Rename the downloaded file\n",
    "            os.rename(old_file_path, new_file_path)\n",
    "            return new_file_path\n",
    "\n",
    "            # excel_data = pd.read_excel(new_file_path)\n",
    "            # print(excel_data)\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading or renaming the Excel file: {e}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    new_file_path = main()\n",
    "    print(f\"New file path: {new_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openpyxl\n",
    "from googleapiclient.discovery import build\n",
    "from google_auth_oauthlib.flow import InstalledAppFlow\n",
    "from google.auth.transport.requests import Request\n",
    "import datetime\n",
    "import pickle\n",
    "import os.path\n",
    "import base64\n",
    "import pandas as pd\n",
    "from io import BytesIO\n",
    "import os\n",
    "import openpyxl\n",
    "import mysql.connector\n",
    "from sshtunnel import SSHTunnelForwarder\n",
    "import pymysql\n",
    "import mysql.connector\n",
    "from requests.structures import CaseInsensitiveDict\n",
    "import requests\n",
    "from Constants import products_matched, dates_22_23, dates_23_24\n",
    "\n",
    "# Load the Excel workbook\n",
    "workbook = openpyxl.load_workbook(new_file_path, data_only=True)\n",
    "\n",
    "# Select the sheet you want to extract data from\n",
    "sheet = workbook['OIL']\n",
    "\n",
    "\n",
    "column_names = []\n",
    "\n",
    "\n",
    "#Connect to mysql database to insert data retrieved from a file sent by an email \n",
    "b_ssh_host = 'XXXX'\n",
    "b_ssh_user = 'XXXX'\n",
    "b_ssh_port = 'XXXX'\n",
    "b_ssh_private_key = 'XXXX'\n",
    "b_sql_hostname = 'XXXX'\n",
    "b_sql_username = 'XXXX'\n",
    "b_sql_password = 'XXXX'\n",
    "b_sql_database = 'XXXX'\n",
    "b_sql_port = 'XXXX'\n",
    "\n",
    "with SSHTunnelForwarder(\n",
    "        (b_ssh_host, b_ssh_port),\n",
    "        ssh_username=b_ssh_user,\n",
    "        ssh_pkey=b_ssh_private_key,\n",
    "        remote_bind_address=(b_sql_hostname, b_sql_port)) as tunnel:\n",
    "\n",
    "    b_conn = mysql.connector.connect(\n",
    "        host='127.0.0.1',\n",
    "        user=b_sql_username,\n",
    "        passwd=b_sql_password,\n",
    "        db=b_sql_database,\n",
    "        port=tunnel.local_bind_port\n",
    "    )\n",
    "\n",
    "    b_cursor = b_conn.cursor()\n",
    "\n",
    "    # Iterate through headers to get column names\n",
    "    for headers in sheet.iter_rows(min_row=3, max_row=3, min_col=3):\n",
    "        for cell in headers:\n",
    "            header_value = cell.value\n",
    "            if header_value is not None and len(str(header_value)) < 4:\n",
    "                column_names.append(header_value)\n",
    "\n",
    "    cell_value_above = None\n",
    "\n",
    "    # Iterate through rows starting from row 4 (assuming data starts from row 4)\n",
    "    for row_num in range(4, 15, 2):  # Increment by 2 to process two rows at a time\n",
    "        product_name_in_sheet = sheet.cell(row=row_num, column=1).value  # Get the product name in the current row\n",
    "\n",
    "        #match product names from the excel file with product ids in a database\n",
    "        check_data_query = \"SELECT * FROM TradeData WHERE product_id = %s AND date = %s AND trade_quantity = %s AND origin_country_id = %s AND destination_country_id_raw = %s AND data_source_id = %s\"\n",
    "        \n",
    "        if product_name_in_sheet is None:\n",
    "            product_name = cell_value_above\n",
    "        else:\n",
    "            product_name = product_name_in_sheet\n",
    "\n",
    "        # Extract product ID based on product name\n",
    "        product_id = products_matched.get(product_name, None)\n",
    "\n",
    "        if product_id is None:\n",
    "            continue\n",
    "\n",
    "        corrected_date = None\n",
    "\n",
    "        def correct_date_1(header_value):\n",
    "            return dates_23_24.get(header_value, None)\n",
    "\n",
    "        def correct_date_2(header_value):\n",
    "            return dates_22_23.get(header_value, None)\n",
    "\n",
    "        # Initialize dictionaries to store data values for selected columns in two rows\n",
    "        data_values_row1 = {}\n",
    "        data_values_row2 = {}\n",
    "\n",
    "        # Iterate through columns in the header row\n",
    "        for i, column in enumerate(sheet.iter_cols(min_row=3, max_row=3, min_col=3), start=3):\n",
    "            header_value = column[0].value  # Get the header value for the current column\n",
    "\n",
    "            if header_value in column_names:\n",
    "                date_str_1 = correct_date_1(header_value)\n",
    "                date_str_2 = correct_date_2(header_value)\n",
    "\n",
    "                if date_str_1 and date_str_2:\n",
    "                    data_values_row1[date_str_1] = sheet.cell(row=row_num, column=i - 0).value\n",
    "                    data_values_row2[date_str_2] = sheet.cell(row=row_num + 1, column=i - 0).value\n",
    "\n",
    "                    if data_values_row1[date_str_1] is not None:\n",
    "\n",
    "                        corrected_data_values_row1 = data_values_row1[date_str_1] * 1000 \n",
    "                        corrected_data_values_row2 = data_values_row2[date_str_2] * 1000 \n",
    "\n",
    "                        \n",
    "                        val = (product_id, date_str_1, corrected_data_values_row1, 150, 0, 5) \n",
    "                        b_cursor.execute(check_data_query, val)\n",
    "                        existing_data = b_cursor.fetchone() \n",
    "\n",
    "                        b_cursor.fetchall()\n",
    "\n",
    "                        if existing_data:\n",
    "                            print(\"Data already exists for\", date_str_1, \"and\", product_id)\n",
    "                            continue\n",
    "                            \n",
    "                        else: \n",
    "                            insert_query = \"INSERT IGNORE INTO TradeData (product_id, date, raw_trade_quantity, destination_country_id_raw, origin_country_id, trade_quantity, raw_unit, unit, data_source_id, frequency) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s)\"\n",
    "                            b_cursor.execute(insert_query,(product_id, date_str_1, data_values_row1[date_str_1], 0 , 150, corrected_data_values_row1, 'kmt', 'mt', 5, 'monthly'))\n",
    "                        \n",
    "                        b_conn.commit()\n",
    "\n",
    "\n",
    "    b_cursor.close()\n",
    "    b_conn.close()\n",
    "\n",
    "print(b_cursor.rowcount, \"record(s) inserted.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a44830708761a843059adba6d554183630a5ed8b6adc3257bd6953cce1e327da"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
