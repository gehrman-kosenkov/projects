{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iskandar:All months in the period are present.\n",
      "Japura:All months in the period are present.\n",
      "Nunukan:All months in the period are present.\n",
      "Oesman:All months in the period are present.\n",
      "Sumatera Selatan:All months in the period are present.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from Constants import stations_mapping\n",
    "\n",
    "def clean_excel_files(directory,station_name):\n",
    "    all_data = []\n",
    "    files = [f for f in os.listdir(directory) if f.endswith('.xlsx') and not f.startswith('~$')]\n",
    "    \n",
    "    for file in files:\n",
    "        file_path = os.path.join(directory, file)\n",
    "        try:\n",
    "            df = pd.read_excel(file_path, engine='openpyxl')\n",
    "        except Exception as e:\n",
    "            print(f\"Skipping file {file} due to error: {e}\")\n",
    "            continue\n",
    "        \n",
    "        # Remove rows 1-8 as they are empty\n",
    "        df = df.iloc[8:]\n",
    "\n",
    "        mask_8888 = df == 8888\n",
    "        mask_9999 = df == 9999\n",
    "        \n",
    "        # Sets the values to pd.NA where the mask is True.\n",
    "        df = df.mask(mask_8888 | mask_9999, pd.NA)\n",
    "        \n",
    "        # Select columns B to J (assuming 0-based index, so columns 1 to 9)\n",
    "        columns_to_convert = df.columns[1:10]\n",
    "        \n",
    "        # Convert these specific columns to numeric, setting errors='coerce' to handle non-numeric data\n",
    "        df[columns_to_convert] = df[columns_to_convert].apply(pd.to_numeric, errors='coerce')\n",
    "        \n",
    "        # Interpolate the selected columns\n",
    "        df[columns_to_convert] = df[columns_to_convert].interpolate(axis=0)\n",
    "\n",
    "\n",
    "        # Convert column A to datetime, setting errors='coerce' to handle non-date entries\n",
    "        df[df.columns[0]] = pd.to_datetime(df[df.columns[0]], format='%d-%m-%Y', errors='coerce')\n",
    "        \n",
    "        # Drop rows where the first column (dates) is NaT (Not a Time)\n",
    "        df = df.dropna(subset=[df.columns[0]])\n",
    "        \n",
    "        # Rename the first column to 'Date'\n",
    "        df = df.rename(columns={\n",
    "            df.columns[0]: 'Date',          \n",
    "            df.columns[1]: f'Tn({station_name})',           \n",
    "            df.columns[2]: f'Tx({station_name})',\n",
    "            df.columns[3]: f'Tavg({station_name})',          \n",
    "            df.columns[4]: f'RH_avg({station_name})',           \n",
    "            df.columns[5]: f'RR({station_name})',\n",
    "            df.columns[6]: f'ss({station_name})',          \n",
    "            df.columns[7]: f'ff_x({station_name})',           \n",
    "            df.columns[8]: f'ddd_x({station_name})',\n",
    "            df.columns[9]: f'ff_avg({station_name})',           \n",
    "            df.columns[10]: f'ddd_car({station_name})'\n",
    "        })\n",
    "        \n",
    "        # Reset index\n",
    "        df = df.reset_index(drop=True)\n",
    "        \n",
    "        all_data.append(df)\n",
    "\n",
    "        df.drop(df.columns[10], axis=1, inplace=True)\n",
    "    \n",
    "    if not all_data:\n",
    "        print(\"No valid Excel files found.\")\n",
    "        return None\n",
    "    \n",
    "    # Merge all data\n",
    "    merged_df = pd.concat(all_data, ignore_index=True)\n",
    "    \n",
    "    # Check if 'Date' column exists\n",
    "    if 'Date' not in merged_df.columns:\n",
    "        print(\"The 'Date' column is missing in the merged DataFrame.\")\n",
    "        return None\n",
    "    \n",
    "    # Sort by Date\n",
    "    merged_df = merged_df.sort_values(by='Date')\n",
    "    \n",
    "    # Reset index after sorting\n",
    "    merged_df = merged_df.reset_index(drop=True)\n",
    "\n",
    "    merged_df = merged_df.drop_duplicates(subset=['Date'])\n",
    "    \n",
    "    # Check for all months in the period 2016.01 - 2024.05\n",
    "    start_date = datetime(2016, 1, 1)\n",
    "    end_date = datetime(2024, 5, 31)\n",
    "    date_range = pd.date_range(start=start_date, end=end_date, freq='M')\n",
    "    date_range_set = set(date_range.strftime('%Y-%m'))\n",
    "    \n",
    "    data_months = merged_df['Date'].dt.strftime('%Y-%m').unique()\n",
    "    data_months_set = set(data_months)\n",
    "    \n",
    "    missing_months = date_range_set - data_months_set\n",
    "    \n",
    "    if missing_months:\n",
    "        print(f\"{station_name}: missing months: {sorted(list(missing_months))}\")\n",
    "    else:\n",
    "        print(f\"{station_name}:All months in the period are present.\")\n",
    "    \n",
    "    return merged_df\n",
    "\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        for station_name, directory in stations_mapping.items():\n",
    "            merged_data = clean_excel_files(directory,station_name)\n",
    "            if merged_data is not None:\n",
    "                # Save the merged data to a new Excel file\n",
    "                merged_data.to_excel(f'merged_data_{station_name}.xlsx', index=False)\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13 (main, Aug 25 2022, 18:29:29) \n[Clang 12.0.0 ]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a44830708761a843059adba6d554183630a5ed8b6adc3257bd6953cce1e327da"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
