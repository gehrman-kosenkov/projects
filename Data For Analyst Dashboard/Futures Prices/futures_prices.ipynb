{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sshtunnel import SSHTunnelForwarder\n",
    "import pandas as pd\n",
    "import pymysql\n",
    "from datetime import datetime, timedelta\n",
    "import traceback\n",
    "from dateutil import relativedelta\n",
    "import json as json \n",
    "\n",
    "\n",
    "a_ssh_host = X\n",
    "a_ssh_user = X\n",
    "a_ssh_port = X\n",
    "a_ssh_private_key = X\n",
    "a_sql_hostname = X\n",
    "a_sql_username = X\n",
    "a_sql_password = X\n",
    "a_sql_database = X\n",
    "a_sql_port = X\n",
    " \n",
    "b_ssh_host = X\n",
    "b_ssh_user = X\n",
    "b_ssh_port = X\n",
    "b_ssh_private_key = X\n",
    "b_sql_hostname = X\n",
    "b_sql_username = X\n",
    "b_sql_password = X\n",
    "b_sql_database = X\n",
    "b_sql_port = X\n",
    "\n",
    "def query_data(ssh_host, ssh_user, ssh_port, ssh_private_key, sql_hostname, sql_username, sql_password, sql_database, sql_port, query):\n",
    "    with SSHTunnelForwarder(\n",
    "            (ssh_host, ssh_port),\n",
    "            ssh_username=ssh_user,\n",
    "            ssh_pkey=ssh_private_key,\n",
    "            remote_bind_address=(sql_hostname, sql_port)) as tunnel:\n",
    "        conn = pymysql.connect(\n",
    "            host='`x`',\n",
    "            user=sql_username,\n",
    "            passwd=sql_password,\n",
    "            db=sql_database,\n",
    "            port=tunnel.local_bind_port\n",
    "        )\n",
    "        data = pd.read_sql_query(query, conn)\n",
    "        conn.close()\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from pymysql import IntegrityError, OperationalError\n",
    "from sshtunnel import SSHTunnelForwarder\n",
    "import pymysql\n",
    "import datetime\n",
    "\n",
    "\n",
    "# Configure logging\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)  # Reduce logging level to minimize overhead\n",
    "\n",
    "# File handler to log detailed debug info\n",
    "file_handler = logging.FileHandler('debug.log')\n",
    "file_handler.setLevel(logging.DEBUG)\n",
    "file_formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "file_handler.setFormatter(file_formatter)\n",
    "\n",
    "# Console handler to log only errors or higher\n",
    "console_handler = logging.StreamHandler()\n",
    "console_handler.setLevel(logging.ERROR)\n",
    "console_formatter = logging.Formatter('%(levelname)s - %(message)s')\n",
    "console_handler.setFormatter(console_formatter)\n",
    "\n",
    "# Add handlers to the logger\n",
    "logger.addHandler(file_handler)\n",
    "logger.addHandler(console_handler)\n",
    "\n",
    "# Define the batch size\n",
    "BATCH_SIZE = 1000  # Adjust the batch size based on your needs\n",
    "\n",
    "def chunker(seq, size):\n",
    "    \"\"\"Generator to divide data into chunks.\"\"\"\n",
    "    for pos in range(0, len(seq), size):\n",
    "        yield seq[pos:pos + size]\n",
    "\n",
    "try:\n",
    "    with SSHTunnelForwarder(\n",
    "            (b_ssh_host, b_ssh_port),\n",
    "            ssh_username=b_ssh_user,\n",
    "            ssh_pkey=b_ssh_private_key,\n",
    "            remote_bind_address=(b_sql_hostname, b_sql_port)) as tunnel:\n",
    "        \n",
    "        logger.info(\"SSH Tunnel established successfully.\")\n",
    "        \n",
    "        try:\n",
    "            b_conn = pymysql.connect(\n",
    "                host='127.0.0.1',\n",
    "                user=b_sql_username,\n",
    "                passwd=b_sql_password,\n",
    "                db=b_sql_database,\n",
    "                port=tunnel.local_bind_port\n",
    "            )\n",
    "            logger.info(\"Database connection established successfully.\")\n",
    "            b_cursor = b_conn.cursor()\n",
    "\n",
    "            try:\n",
    "                query_1 = '''SELECT f.settlement, settlement_time, fc.maturity, JSON_UNQUOTE(JSON_EXTRACT(ds.properties, '$.financial_product_id')) AS financial_product_id,\n",
    "                            JSON_UNQUOTE(JSON_EXTRACT(ds.properties, '$.exchange_id')) AS exchange_id\n",
    "                                    FROM futures AS f\n",
    "                                    JOIN data_series_new AS ds ON f.data_series_id = ds.id\n",
    "                                    JOIN futures_contracts AS fc ON fc.id = JSON_UNQUOTE(JSON_EXTRACT(ds.properties, '$.futures_contract_id'))\n",
    "                                    WHERE JSON_UNQUOTE(JSON_EXTRACT(ds.properties, '$.financial_product_id')) IN (77,89,3,76,53,55,69)\n",
    "                                    AND fc.maturity >= DATE(CONCAT(YEAR(CURDATE()), '-', LPAD(MONTH(CURDATE()), 2, '0'), '-01'))\n",
    "                                    AND settlement IS NOT NULL\n",
    "                                    AND fc.maturity <= DATE(CONCAT(YEAR(DATE_ADD(CURDATE(), INTERVAL 2 MONTH)), '-', LPAD(MONTH(DATE_ADD(CURDATE(), INTERVAL 2 MONTH)), 2, '0'), '-01'))\n",
    "                            '''\n",
    "                \n",
    "                logger.info(f\"Executing query: {query_1}\")\n",
    "                results = query_data(a_ssh_host, a_ssh_user, a_ssh_port, a_ssh_private_key,\n",
    "                                     a_sql_hostname, a_sql_username, a_sql_password, a_sql_database, a_sql_port, query_1)\n",
    "                logger.info(f\"Query executed successfully, retrieved {len(results)} rows.\")\n",
    "\n",
    "                inserting_query = '''INSERT IGNORE INTO futures_prices\n",
    "                                     (exchange_id, maturity_date, financial_product_id,currency,unit,settlement, settlement_date, source_id)\n",
    "                                     VALUES (%s, %s, %s, %s, %s, %s, %s, %s)'''\n",
    "                \n",
    "                results['settlement_time'] = pd.to_datetime(results['settlement_time'])   \n",
    "                results['settlement_date'] = results['settlement_time'].dt.date\n",
    "                results = results.sort_values(by=['financial_product_id', 'settlement_date', 'settlement_time'], ascending=[True, True, False])\n",
    "\n",
    "                results_modified = results.drop_duplicates(subset=['financial_product_id', 'settlement_date'])\n",
    "\n",
    "                currency_unit_mapping = {\n",
    "                        '77': ('EUR', 'mt'),\n",
    "                        '89': ('USD', 'mt'),\n",
    "                        '3': ('USD', 'barrel'),\n",
    "                        '76': ('MYR', 'mt'),\n",
    "                        '53': ('USD cents', 'bushel'),\n",
    "                        '55': ('USD cents', 'lb')\n",
    "                    }\n",
    "\n",
    "                source_id = 1\n",
    "\n",
    "\n",
    "            \n",
    "\n",
    "                values = [\n",
    "                            (\n",
    "                                item['exchange_id'],\n",
    "                                item['maturity'],\n",
    "                                item['financial_product_id'],\n",
    "                                currency_unit_mapping.get(item['financial_product_id'], ('', ''))[0],  # currency\n",
    "                                currency_unit_mapping.get(item['financial_product_id'], ('', ''))[1],  # unit\n",
    "                                item['settlement'],\n",
    "                                item['settlement_date'],\n",
    "                                source_id\n",
    "                            )\n",
    "                            for index, item in results_modified.iterrows()\n",
    "                        ]\n",
    "\n",
    "\n",
    "                # Insert in batches\n",
    "                for i, chunk in enumerate(chunker(values, BATCH_SIZE)):\n",
    "                    logger.info(f\"Inserting batch {i + 1} of {len(values) // BATCH_SIZE + 1}\")\n",
    "                    b_cursor.executemany(inserting_query, chunk)\n",
    "                    b_conn.commit()\n",
    "                    logger.info(f\"Batch {i + 1} committed successfully.\")\n",
    "\n",
    "            except IntegrityError as ie:\n",
    "                logger.error(f\"Integrity error occurred: {ie}\")\n",
    "                b_conn.rollback()\n",
    "                logger.info(\"Transaction rolled back due to IntegrityError.\")\n",
    "\n",
    "            except Exception as e:\n",
    "                logger.error(f\"An unexpected error occurred during query execution: {e}\")\n",
    "                b_conn.rollback()\n",
    "                logger.info(\"Transaction rolled back due to an unexpected error.\")\n",
    "\n",
    "            finally:\n",
    "                b_cursor.close()\n",
    "                logger.info(\"Cursor closed.\")\n",
    "\n",
    "        except OperationalError as oe:\n",
    "            logger.error(f\"Operational error occurred: {oe}\")\n",
    "\n",
    "        finally:\n",
    "            b_conn.close()\n",
    "            logger.info(\"Database connection closed.\")\n",
    "\n",
    "except Exception as e:\n",
    "    logger.critical(f\"Critical error in establishing SSH Tunnel: {e}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a44830708761a843059adba6d554183630a5ed8b6adc3257bd6953cce1e327da"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
