{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New file path: /Users/germankosenkov/Code projects/Crawling/3. Crawling Emails/New/RUS_exports/2024_Week_8.xlsx\n"
     ]
    }
   ],
   "source": [
    "from googleapiclient.discovery import build\n",
    "from google_auth_oauthlib.flow import InstalledAppFlow\n",
    "from google.auth.transport.requests import Request\n",
    "import datetime\n",
    "import pickle\n",
    "import os.path\n",
    "import base64\n",
    "import pandas as pd\n",
    "from io import BytesIO\n",
    "import os\n",
    "import openpyxl\n",
    "import mysql.connector\n",
    "from sshtunnel import SSHTunnelForwarder\n",
    "import pymysql\n",
    "import mysql.connector\n",
    "from requests.structures import CaseInsensitiveDict\n",
    "import requests\n",
    "\n",
    "# If modifying these SCOPES, delete the file token.pickle.\n",
    "SCOPES = ['https://www.googleapis.com/auth/gmail.readonly']\n",
    "\n",
    "def get_gmail_service():\n",
    "    creds = None\n",
    "    # The file token.pickle stores the user's access and refresh tokens.\n",
    "    if os.path.exists('token.pickle'):\n",
    "        with open('token.pickle', 'rb') as token:\n",
    "            creds = pickle.load(token)\n",
    "    # If there are no (valid) credentials available, let the user log in.\n",
    "    if not creds or not creds.valid:\n",
    "        if creds and creds.expired and creds.refresh_token:\n",
    "            creds.refresh(Request())\n",
    "        else:\n",
    "            flow = InstalledAppFlow.from_client_secrets_file(\n",
    "                '/Users/germankosenkov/Code projects/Crawling/3. Crawling Emails/New/client_secret_811893502700-tj4c2bv2942q1ua3459au95sc1ohoqle.apps.googleusercontent.com.json', SCOPES)\n",
    "            creds = flow.run_local_server(port=0)\n",
    "        # Save the credentials for the next run\n",
    "        with open('token.pickle', 'wb') as token:\n",
    "            pickle.dump(creds, token)\n",
    "\n",
    "    service = build('gmail', 'v1', credentials=creds)\n",
    "    return service\n",
    "\n",
    "def search_messages(service, query):\n",
    "    result = service.users().messages().list(userId='me', q=query).execute()\n",
    "    messages = []\n",
    "    if 'messages' in result:\n",
    "        messages.extend(result['messages'])\n",
    "    return messages\n",
    "\n",
    "def get_attachments(service, message_id, store_dir, desired_filename):\n",
    "    try:\n",
    "        message = service.users().messages().get(userId='me', id=message_id).execute()\n",
    "        parts = message.get('payload', {}).get('parts', [])\n",
    "        for part in parts:\n",
    "            if part['filename'] == desired_filename:  # Check if the filename matches the desired one\n",
    "                if 'data' in part['body']:\n",
    "                    data = part['body']['data']\n",
    "                else:\n",
    "                    att_id = part['body'].get('attachmentId')\n",
    "                    att = service.users().messages().attachments().get(userId='me', messageId=message_id, id=att_id).execute()\n",
    "                    data = att['data']\n",
    "\n",
    "                file_data = base64.urlsafe_b64decode(data.encode('UTF-8'))\n",
    "                path = os.path.join(store_dir, part['filename'])\n",
    "\n",
    "                if not os.path.exists(store_dir):\n",
    "                    os.makedirs(store_dir)\n",
    "                with open(path, 'wb') as f:\n",
    "                    f.write(file_data)\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "\n",
    "current_date = datetime.date.today()\n",
    "current_week = current_date.isocalendar()[1]\n",
    "previous_week = current_week - 1\n",
    "\n",
    "current_year = datetime.date.today().year\n",
    "\n",
    "new_file_path = ''\n",
    "\n",
    "\n",
    "def main():\n",
    "    service = get_gmail_service()\n",
    "    search_query = 'from:XXXX.org has:attachment filename:XXXX.xlsx'\n",
    "    messages = search_messages(service, search_query)\n",
    "\n",
    "    if messages:\n",
    "        latest_message = messages[0]  # Get the latest email\n",
    "\n",
    "        # Construct the path for saving the attachment with the week and year in the file name\n",
    "        #folder_name = f'{current_year}_Week_{previous_week}'\n",
    "        save_path = '/Users/germankosenkov/Code projects/Crawling/3. Crawling Emails/New/XXX_exports'\n",
    "        \n",
    "\n",
    "        # Call get_attachments with the desired filename\n",
    "        get_attachments(service, latest_message['id'], save_path, 'XXXX.xlsx')\n",
    "\n",
    "        # Path to the downloaded Excel file\n",
    "        old_file_path = os.path.join(save_path, 'XXXX.xlsx')\n",
    "        new_file_path = os.path.join(save_path, f'{current_year}_Week_{previous_week}.xlsx')\n",
    "\n",
    "        try:\n",
    "            # Rename the downloaded file\n",
    "            os.rename(old_file_path, new_file_path)\n",
    "            return new_file_path\n",
    "\n",
    "            # excel_data = pd.read_excel(new_file_path)\n",
    "            # print(excel_data)\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading or renaming the Excel file: {e}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    new_file_path = main()\n",
    "    print(f\"New file path: {new_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data already exists for 2023-09-30 and 263\n",
      "Data already exists for 2023-10-31 and 263\n",
      "Data already exists for 2023-11-30 and 263\n",
      "Data already exists for 2023-12-31 and 263\n",
      "Data already exists for 2024-01-31 and 263\n",
      "Data already exists for 2023-09-30 and 859\n",
      "Data already exists for 2023-10-31 and 859\n",
      "Data already exists for 2023-11-30 and 859\n",
      "Data already exists for 2023-12-31 and 859\n",
      "Data already exists for 2023-09-30 and 846\n",
      "Data already exists for 2023-10-31 and 846\n",
      "Data already exists for 2023-11-30 and 846\n",
      "Data already exists for 2023-12-31 and 846\n",
      "Data already exists for 2024-01-31 and 846\n",
      "Data already exists for 2023-09-30 and 244\n",
      "Data already exists for 2023-10-31 and 244\n",
      "Data already exists for 2023-11-30 and 244\n",
      "Data already exists for 2023-12-31 and 244\n",
      "Data already exists for 2024-01-31 and 244\n",
      "Data already exists for 2023-09-30 and 93\n",
      "Data already exists for 2023-10-31 and 93\n",
      "Data already exists for 2023-11-30 and 93\n",
      "Data already exists for 2023-12-31 and 93\n",
      "Data already exists for 2024-01-31 and 93\n",
      "Data already exists for 2023-09-30 and 92\n",
      "Data already exists for 2023-10-31 and 92\n",
      "Data already exists for 2023-11-30 and 92\n",
      "Data already exists for 2023-12-31 and 92\n",
      "Data already exists for 2024-01-31 and 92\n",
      "-1 record(s) inserted.\n"
     ]
    }
   ],
   "source": [
    "import openpyxl\n",
    "from googleapiclient.discovery import build\n",
    "from google_auth_oauthlib.flow import InstalledAppFlow\n",
    "from google.auth.transport.requests import Request\n",
    "import datetime\n",
    "import pickle\n",
    "import os.path\n",
    "import base64\n",
    "import pandas as pd\n",
    "from io import BytesIO\n",
    "import os\n",
    "import openpyxl\n",
    "import mysql.connector\n",
    "from sshtunnel import SSHTunnelForwarder\n",
    "import pymysql\n",
    "import mysql.connector\n",
    "from requests.structures import CaseInsensitiveDict\n",
    "import requests\n",
    "from Constants import products_matched, dates_22_23, dates_23_24\n",
    "\n",
    "# Load the Excel workbook\n",
    "workbook = openpyxl.load_workbook(new_file_path, data_only=True)\n",
    "\n",
    "# Select the sheet you want to extract data from\n",
    "sheet = workbook['OIL']\n",
    "\n",
    "\n",
    "column_names = []\n",
    "\n",
    "\n",
    "b_ssh_host = 'XXXX'\n",
    "b_ssh_user = 'XXXX'\n",
    "b_ssh_port = 'XXXX'\n",
    "b_ssh_private_key = 'XXXX'\n",
    "b_sql_hostname = 'XXXX'\n",
    "b_sql_username = 'XXXX'\n",
    "b_sql_password = 'XXXX'\n",
    "b_sql_database = 'XXXX'\n",
    "b_sql_port = 'XXXX'\n",
    "\n",
    "with SSHTunnelForwarder(\n",
    "        (b_ssh_host, b_ssh_port),\n",
    "        ssh_username=b_ssh_user,\n",
    "        ssh_pkey=b_ssh_private_key,\n",
    "        remote_bind_address=(b_sql_hostname, b_sql_port)) as tunnel:\n",
    "\n",
    "    b_conn = mysql.connector.connect(\n",
    "        host='127.0.0.1',\n",
    "        user=b_sql_username,\n",
    "        passwd=b_sql_password,\n",
    "        db=b_sql_database,\n",
    "        port=tunnel.local_bind_port\n",
    "    )\n",
    "\n",
    "    b_cursor = b_conn.cursor()\n",
    "\n",
    "    # Iterate through headers to get column names\n",
    "    for headers in sheet.iter_rows(min_row=3, max_row=3, min_col=3):\n",
    "        for cell in headers:\n",
    "            header_value = cell.value\n",
    "            if header_value is not None and len(str(header_value)) < 4:\n",
    "                column_names.append(header_value)\n",
    "\n",
    "    cell_value_above = None\n",
    "\n",
    "    # Iterate through rows starting from row 4 (assuming data starts from row 4)\n",
    "    for row_num in range(4, 15, 2):  # Increment by 2 to process two rows at a time\n",
    "        product_name_in_sheet = sheet.cell(row=row_num, column=1).value  # Get the product name in the current row\n",
    "\n",
    "\n",
    "\n",
    "        check_data_query = \"SELECT * FROM TradeData WHERE product_id = %s AND date = %s AND trade_quantity = %s AND origin_country_id = %s AND destination_country_id_raw = %s AND data_source_id = %s\"\n",
    "        \n",
    "        if product_name_in_sheet is None:\n",
    "            product_name = cell_value_above\n",
    "        else:\n",
    "            product_name = product_name_in_sheet\n",
    "\n",
    "        # Extract product ID based on product name\n",
    "        product_id = products_matched.get(product_name, None)\n",
    "\n",
    "        if product_id is None:\n",
    "            continue\n",
    "\n",
    "        corrected_date = None\n",
    "\n",
    "        def correct_date_1(header_value):\n",
    "            return dates_23_24.get(header_value, None)\n",
    "\n",
    "        def correct_date_2(header_value):\n",
    "            return dates_22_23.get(header_value, None)\n",
    "\n",
    "        # Initialize dictionaries to store data values for selected columns in two rows\n",
    "        data_values_row1 = {}\n",
    "        data_values_row2 = {}\n",
    "\n",
    "        # Iterate through columns in the header row\n",
    "        for i, column in enumerate(sheet.iter_cols(min_row=3, max_row=3, min_col=3), start=3):\n",
    "            header_value = column[0].value  # Get the header value for the current column\n",
    "\n",
    "            if header_value in column_names:\n",
    "                date_str_1 = correct_date_1(header_value)\n",
    "                date_str_2 = correct_date_2(header_value)\n",
    "\n",
    "                if date_str_1 and date_str_2:\n",
    "                    data_values_row1[date_str_1] = sheet.cell(row=row_num, column=i - 0).value\n",
    "                    data_values_row2[date_str_2] = sheet.cell(row=row_num + 1, column=i - 0).value\n",
    "\n",
    "                    if data_values_row1[date_str_1] is not None:\n",
    "\n",
    "                        corrected_data_values_row1 = data_values_row1[date_str_1] * 1000 \n",
    "                        corrected_data_values_row2 = data_values_row2[date_str_2] * 1000 \n",
    "\n",
    "                        \n",
    "                        val = (product_id, date_str_1, corrected_data_values_row1, 150, 0, 5) \n",
    "                        b_cursor.execute(check_data_query, val)\n",
    "                        existing_data = b_cursor.fetchone() \n",
    "\n",
    "                        b_cursor.fetchall()\n",
    "\n",
    "                        if existing_data:\n",
    "                            print(\"Data already exists for\", date_str_1, \"and\", product_id)\n",
    "                            continue\n",
    "                            \n",
    "                        else: \n",
    "                            insert_query = \"INSERT IGNORE INTO TradeData (product_id, date, raw_trade_quantity, destination_country_id_raw, origin_country_id, trade_quantity, raw_unit, unit, data_source_id, frequency) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s)\"\n",
    "                            b_cursor.execute(insert_query,(product_id, date_str_1, data_values_row1[date_str_1], 0 , 150, corrected_data_values_row1, 'kmt', 'mt', 5, 'monthly'))\n",
    "                        \n",
    "                        b_conn.commit()\n",
    "\n",
    "\n",
    "    b_cursor.close()\n",
    "    b_conn.close()\n",
    "\n",
    "print(b_cursor.rowcount, \"record(s) inserted.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a44830708761a843059adba6d554183630a5ed8b6adc3257bd6953cce1e327da"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
