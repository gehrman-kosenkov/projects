{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#taking out the defintion for successful results\n",
    "\n",
    "import asyncio\n",
    "import aiohttp\n",
    "import pandas as pd\n",
    "import nest_asyncio\n",
    "import time\n",
    "from constants import taxud_mapping, GTT_mapping, headers\n",
    "\n",
    "# Apply nest_asyncio to allow nested use of asyncio.run()\n",
    "nest_asyncio.apply()\n",
    "\n",
    "MAX_RETRIES = 5\n",
    "RETRY_BACKOFF_FACTOR = 1.5\n",
    "BATCH_SIZE = 10  # Number of requests to send in a batch\n",
    "DELAY_BETWEEN_BATCHES = 5  # Delay in seconds between batches\n",
    "\n",
    "successful_results = []\n",
    "failed_results = []\n",
    "\n",
    "async def fetch_data(session, product):\n",
    "    sector = product[\"sector\"]\n",
    "    HS_codes = product[\"HS codes\"]\n",
    "    product_name = product[\"name\"]\n",
    "\n",
    "    url = f\"https://www.ec.europa.eu/agrifood/api/taxud/weeklyData/import?importCategories=Import%20-%20preferential&importCategories=Import%20-%20most favoured nation&sectors={sector}&{HS_codes}\"\n",
    "\n",
    "    for attempt in range(MAX_RETRIES):\n",
    "        try:\n",
    "            async with session.get(url, headers=headers) as response:\n",
    "                if response.status == 200:\n",
    "                    data = await response.json()\n",
    "                    df = pd.DataFrame(data)\n",
    "                    df.rename(columns={'kgEquivalent': f'TAXUD_{product_name}'}, inplace=True)\n",
    "\n",
    "                    return df, None  # Return DataFrame and None as error\n",
    "                elif response.status >= 500:  # Server error, retry\n",
    "                    print(f\"Server error (HTTP {response.status}) for {product_name}, retrying...\")\n",
    "                else:  # Client error, do not retry\n",
    "                    return None, f\"Failed to retrieve data for {product_name}: HTTP {response.status}\"\n",
    "        except aiohttp.ClientError as e:\n",
    "            print(f\"Client error for {product_name}, retrying: {e}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Unexpected error for {product_name}, retrying: {e}\")\n",
    "\n",
    "        await asyncio.sleep(RETRY_BACKOFF_FACTOR ** attempt)\n",
    "\n",
    "    return None, f\"Failed to retrieve data for {product_name} after {MAX_RETRIES} attempts\"\n",
    "\n",
    "async def fetch_batch(session, batch):\n",
    "    tasks = [fetch_data(session, product) for product in batch]\n",
    "    return await asyncio.gather(*tasks)\n",
    "\n",
    "async def main():\n",
    "    global successful_results, failed_results\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        results = []\n",
    "        for i in range(0, len(taxud_mapping), BATCH_SIZE):\n",
    "            batch = taxud_mapping[i:i + BATCH_SIZE]\n",
    "            results.extend(await fetch_batch(session, batch))\n",
    "            await asyncio.sleep(DELAY_BETWEEN_BATCHES)\n",
    "\n",
    "    successful_results = [result for result, error in results if result is not None]\n",
    "    failed_results = [error for result, error in results if error is not None]\n",
    "\n",
    "    if successful_results:\n",
    "        print(\"Successfully retrieved data:\")\n",
    "        for df in successful_results:\n",
    "            print(df.head(10))  # Print the first 10 rows of each successfully retrieved DataFrame\n",
    "    else:\n",
    "        print(\"No data retrieved for any product.\")\n",
    "\n",
    "    if failed_results:\n",
    "        print(\"\\nFailed to retrieve data for the following products:\")\n",
    "        for error in failed_results:\n",
    "            print(error)\n",
    "\n",
    "# Run the main function\n",
    "asyncio.run(main())\n",
    "\n",
    "# Use the successful_results in the next block of code\n",
    "print(\"Using the successful_results in the next block of code...\")\n",
    "# Add your code to process successful_results here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "from dateutil import parser\n",
    "\n",
    "\n",
    "#In this code we change data aggregation by turning marketing year (starts in July and ends in June) into a normal calendar year\n",
    "def week_to_date(marketing_year, week):\n",
    "\n",
    "    start_year = int(marketing_year.split('/')[0])\n",
    "    end_year = int(marketing_year.split('/')[1])\n",
    "\n",
    "    if week <= 26: \n",
    "        year = start_year\n",
    "        week_number = week + 26\n",
    "    else:  \n",
    "        year = end_year\n",
    "        week_number = week - 27\n",
    "\n",
    "    jan_1 = datetime(year, 1, 1)\n",
    "    first_week_start = jan_1 + timedelta(days=(7 - jan_1.weekday()))\n",
    "    week_start = first_week_start + timedelta(weeks=week_number - 1)\n",
    "    week_end = week_start + timedelta(days=6)\n",
    "\n",
    "    return week_start, week_end "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "aggregated_results = []\n",
    "\n",
    "for df in successful_results:\n",
    "    df.columns = df.columns.str.strip()\n",
    "\n",
    "    if 'marketingYear' not in df.columns or 'week' not in df.columns:\n",
    "        print(\"DataFrame does not contain 'Marketing Year' or 'week' columns.\")\n",
    "        continue\n",
    "\n",
    "    # Calculate start and end dates for each week\n",
    "    df[['Start date', 'End date']] = df.apply(lambda row: pd.Series(week_to_date(row['marketingYear'], row['week'])), axis=1)\n",
    "\n",
    "    def split_week_into_months(row):\n",
    "        result = []\n",
    "        total_days = (row['End date'] - row['Start date']).days + 1\n",
    "        current_date = row['Start date']\n",
    "        \n",
    "        while current_date <= row['End date']:\n",
    "            start_of_month = current_date.replace(day=1)\n",
    "            end_of_month = (start_of_month + pd.DateOffset(months=1) - pd.DateOffset(days=1)).date()\n",
    "            \n",
    "            if current_date.month == row['End date'].month:\n",
    "                end_of_period = row['End date']\n",
    "            else:\n",
    "                end_of_period = pd.Timestamp(end_of_month)\n",
    "            \n",
    "            days_in_period = (end_of_period - current_date).days + 1\n",
    "            proportion = days_in_period / total_days\n",
    "            \n",
    "            taxud_columns = [col for col in df.columns if col.startswith('TAXUD_')]\n",
    "\n",
    "            for col in taxud_columns:\n",
    "                value_for_period = (row[col] * proportion) / 1000\n",
    "                result.append({\n",
    "                    'date': start_of_month,\n",
    "                    col: value_for_period\n",
    "                })\n",
    "\n",
    "            current_date = end_of_period + pd.DateOffset(days=1)\n",
    "\n",
    "        return result\n",
    "\n",
    "    # Apply the function and expand the results into a new DataFrame\n",
    "    expanded_rows = df.apply(split_week_into_months, axis=1)\n",
    "    expanded_df = pd.DataFrame([item for sublist in expanded_rows for item in sublist])\n",
    "\n",
    "    # Aggregate data by month\n",
    "    monthly_aggregated = expanded_df.groupby(['date']).sum().reset_index()\n",
    "\n",
    "    # Add the aggregated DataFrame to the list\n",
    "    aggregated_results.append(monthly_aggregated)\n",
    "\n",
    "# Merge all aggregated DataFrames on 'month'\n",
    "if aggregated_results:\n",
    "    final_aggregated_df = aggregated_results[0]\n",
    "    for df in aggregated_results[1:]:\n",
    "        final_aggregated_df = pd.merge(final_aggregated_df, df, on='date', how='outer')\n",
    "else:\n",
    "    final_aggregated_df = pd.DataFrame()\n",
    "\n",
    "# Replace NaNs with zeros\n",
    "final_aggregated_df.fillna(0, inplace=True)\n",
    "\n",
    "final_aggregated_df.sort_values(by='date', inplace=True)\n",
    "\n",
    "TAXUD_merged_df = final_aggregated_df\n",
    "\n",
    "\n",
    "\n",
    "# Print the final aggregated DataFrame\n",
    "print(TAXUD_merged_df.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main_script.py\n",
    "import os\n",
    "from database_connection import query_data\n",
    "\n",
    "\n",
    "a_ssh_host = X\n",
    "a_ssh_user = X\n",
    "a_ssh_port = X\n",
    "a_ssh_private_key = X\n",
    "a_sql_hostname = X\n",
    "a_sql_username = X\n",
    "a_sql_password = X\n",
    "a_sql_database = X\n",
    "a_sql_port = X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from database_connection import ssh_tunnel, db_connection, query_data\n",
    "\n",
    "# Load environment variables from .env\n",
    "load_dotenv()\n",
    "\n",
    "dfs_GTT = []\n",
    "\n",
    "with ssh_tunnel(a_ssh_host, a_ssh_port, a_ssh_user, a_ssh_private_key, a_sql_hostname, a_sql_port) as local_port:\n",
    "    with db_connection(local_port, a_sql_username, a_sql_password, a_sql_database) as conn:\n",
    "        for item in GTT_mapping:\n",
    "            GTT_code = item['GTT_code']\n",
    "            name = item['name']\n",
    "\n",
    "            query = f'''SELECT date, amount\n",
    "                        FROM vesper.total_import_export_figures\n",
    "                        WHERE country_id = 200 AND hs_code = {GTT_code} AND data_interval = 'monthly' AND type = 1 AND date > '2010-12-31'\n",
    "                        ORDER BY date ASC'''\n",
    "\n",
    "            result = query_data(conn, query)\n",
    "            if not result.empty:\n",
    "                result['date'] = pd.to_datetime(result['date'])\n",
    "                result = result.rename(columns={'amount': f'GTT_{name}'})\n",
    "                dfs_GTT.append(result)\n",
    "\n",
    "if dfs_GTT:\n",
    "    # Start with the first dataframe\n",
    "    merged_df = dfs_GTT[0]\n",
    "    \n",
    "    # Merge the rest without dropping NaNs\n",
    "    for df in dfs_GTT[1:]:\n",
    "        merged_df = pd.merge(merged_df, df, on='date', how='outer')\n",
    "\n",
    "    # Optional: Fill NaN values with 0 or another value\n",
    "    # merged_df = merged_df.fillna(0)\n",
    "\n",
    "    GTT_merged_df = merged_df\n",
    "\n",
    "    print(GTT_merged_df.head(10))\n",
    "\n",
    "else:\n",
    "    print(\"No dataframes to merge\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TAXUD_and_GTT = pd.merge(TAXUD_merged_df , GTT_merged_df, on='date', how='outer')\n",
    "\n",
    "start_date = '2012-01-01'\n",
    "end_date = '2024-05-31'\n",
    "\n",
    "start_date = pd.to_datetime(start_date)\n",
    "end_date = pd.to_datetime(end_date)\n",
    "\n",
    "filtered_TAXUD_and_GTT = TAXUD_and_GTT[(TAXUD_and_GTT['date'] >= start_date) & (TAXUD_and_GTT['date'] <= end_date)]\n",
    "\n",
    "# Print a summary of the DataFrame\n",
    "print(\"Merged DataFrame head:\")\n",
    "print(filtered_TAXUD_and_GTT.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "\n",
    "# Assume filtered_df is your DataFrame and it's already defined\n",
    "\n",
    "# Extract unique suffixes from the columns\n",
    "suffixes = {col.split('_')[1] for col in filtered_TAXUD_and_GTT.columns if '_' in col}\n",
    "\n",
    "# Ensure date column is in datetime format\n",
    "filtered_TAXUD_and_GTT['date'] = pd.to_datetime(filtered_TAXUD_and_GTT['date'])\n",
    "\n",
    "# Extract year and month for plotting purposes\n",
    "filtered_TAXUD_and_GTT['year'] = filtered_TAXUD_and_GTT['date'].dt.year\n",
    "filtered_TAXUD_and_GTT['month'] = filtered_TAXUD_and_GTT['date'].dt.month\n",
    "\n",
    "# Create a directory to save plots if needed\n",
    "# import os\n",
    "# os.makedirs('plots', exist_ok=True)\n",
    "\n",
    "# Create a PdfPages object\n",
    "with PdfPages('time_series_plots.pdf') as pdf:\n",
    "    # Iterate over each suffix to create separate charts\n",
    "    for suffix in suffixes:\n",
    "        taxud_col = f'TAXUD_{suffix}'\n",
    "        gtt_col = f'GTT_{suffix}'\n",
    "\n",
    "        if taxud_col in filtered_TAXUD_and_GTT.columns and gtt_col in filtered_TAXUD_and_GTT.columns:\n",
    "            plt.figure(figsize=(12, 6))\n",
    "\n",
    "            # Plot TAXUD data\n",
    "            plt.plot(filtered_TAXUD_and_GTT['date'], filtered_TAXUD_and_GTT[taxud_col], label=f'TAXUD_{suffix}', color='blue')\n",
    "\n",
    "            # Plot GTT data\n",
    "            plt.plot(filtered_TAXUD_and_GTT['date'], filtered_TAXUD_and_GTT[gtt_col], label=f'GTT_{suffix}', color='red')\n",
    "\n",
    "            # Set up the title, labels, and legend\n",
    "            plt.title(f'Time Series for {suffix}')\n",
    "            plt.xlabel('Date')\n",
    "            plt.ylabel('Value')\n",
    "            plt.legend()\n",
    "            plt.grid(True)\n",
    "\n",
    "            # Adjust layout and add the plot to the PDF\n",
    "            plt.tight_layout()\n",
    "            pdf.savefig()  # Save the current figure into the PDF\n",
    "            plt.close()  # Close the figure to free memory\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a44830708761a843059adba6d554183630a5ed8b6adc3257bd6953cce1e327da"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
